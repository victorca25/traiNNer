name: debug_001_template # use "debug" or "debug_nochkp" in the name to run a test session and check everything is working. Does validation and state saving every 8 iterations.
#name: 001_template # remove "debug" to run the real training session.
use_tb_logger: true
model: vsrgan
scale: 4
gpu_ids: [0]
use_amp: true

# Dataset options:
datasets:
  train: 
    name: REDS
    mode: VLRHR
    dataroot_HR: '../datasets/train/hr' # high resolution / ground truth images
    dataroot_LR: '../datasets/train/lr' # low resolution images. If there are missing LR images, they will be generated on the fly from HR
    
    subset_file: null
    use_shuffle: true
    znorm: false # true | false # To normalize images in [-1, 1] range. Default = None (range [0,1]). Can use with activation function like tanh.
    n_workers: 4 # 0 to disable CPU multithreading, or an integrer representing CPU threads to use for dataloading
    batch_size: 32
    virtual_batch_size: 32
    HR_size: 32 # patch size. Default: 128. Needs to be coordinated with the patch size of the features network
    image_channels: 3 # number of channels to load images in
    num_frames: 3 # number of frames to use to train the network (default: 3). Must coincide with "n_frames" in the network.
    random_reverse: true # augmentation to randomly revert the frame orders
    max_frameskip: 3 # if training data FPS is higher than inference data, introduce random frameskip between 1 and max_frameskip
    srcolors: true # enable to be able to use losses that need 3 channels (feature losses, CX, etc), otherwise training only with Y channel
    y_only: false #true
    # tensor_shape: CTHW # TCHW (SOFVSR), CTHW (EVSRGAN)

    # If manually configuring on the fly generation of LR: (else, it will automatically default to Matlab-like downscale algorithm (777) when/if required
    dataroot_kernels: '../training/kernels/results/' # location of the image kernels extracted with KernelGAN, for use with the "realistic" downscale type
    lr_downscale: true # false
    lr_downscale_types: ["linear", "cubic", "matlab_bicubic"] # select from ["nearest", "linear", "cubic", "area", "lanczos4", "linear_exact", "matlab_bicubic", "realistic"] #scale_algos #scaling interpolation options

    # Rotations augmentations:
    # use_flip: true # flip images
    # use_rot: true # rotate images in 90 degree angles
    # hr_rrot: false # rotate images in random degress between -45 and 45
    
    # Noise and blur augmentations:
    lr_blur: false # true | false
    lr_blur_types: {gaussian: 1, clean: 3} # select from: "average","box","gaussian","bilateral","clean" ##blur options #median and motion aren't working yet
    noise_data: ../noise_patches/normal/ # location of the noise patches extracted from real images to use for noise injection with noise option "patches"
    lr_noise: false # true | false
    lr_noise_types: {gaussian: 1, JPEG: 1, clean: 4} # select from: "gaussian", "JPEG", "quantize", "poisson", "dither", "s&p", "speckle", "patches", "clean"

path:
    strict: false # true | false 
    root: 'D:/Code/GitHub/BasicSR'
    pretrain_model_G: '../experiments/pretrained_models/video_x4.pth'
    #resume_state: '../experiments/debug_002_video_x4_REDS/training_state/16.state'

# Generator options:
network_G:
    # SOFVSR:
    which_model_G: sofvsr_net
    n_frames: 3 # number of frames the network will use to estimate the central frame (n-1)/2. Must coincide with "num_frames" in the dataset.
    channels: 320 # feature extraction layer with 320 kernels of size 3 Ã— 3
    img_ch: 3 # # of input image channels: 3 for RGB and 1 for grayscale
    # for SR network:
    SR_net: rrdb #sofvsr | rrdb | pan
    sr_nf: 64 # for rrdb or pan
    sr_nb: 23 # for rrdb or pan
    sr_gc: 32 # for rrdb
    # sr_unf: 24 # for pan
    sr_gaussian_noise: true # for rrdb
    sr_plus: false # for rrdb
    # sr_sa: true # for pan
    # sr_upinter_mode: nearest # for pan

    # 3DSR:
    # which_model_G: sr3d_net
    # nf: 64 # Number of discrim filters
    # in_nc: 3 # Number of input image channels: 3 for RGB and 1 for grayscale
    # out_nc: 3 # Number of output image channels: 3 for RGB and 1 for grayscale

    # EVSRGAN:
    # which_model_G: RRDB_net # RRDB_net (original ESRGAN arch)
    # norm_type: null
    # mode: CNA
    # nf: 64 # Number of discrim filters in the first conv layer
    # nb: 23
    # in_nc: 3 # Number of input image channels: 3 for RGB and 1 for grayscale
    # out_nc: 3 # Number of output image channels: 3 for RGB and 1 for grayscale
    # gc: 32
    # group: 1
    # convtype: Conv3D
    # net_act: leakyrelu # swish | leakyrelu
    # gaussian: true # true | false
    # plus: false # true | false
    # ##finalact: tanh # Test. Activation function to make outputs fit in [-1, 1] range. Default = None. Coordinate with znorm.

    # EDVR:
    # which_model_G: EDVR_net
    # in_nc: 3 # Number of input image channels: 3 for RGB and 1 for grayscale
    # out_nc: 3 # Number of output image channels: 3 for RGB and 1 for grayscale
    # nf: 64 # Number of features (M=64, L=128)
    # n_frames: 3 # number of frames the network will use to estimate the central frame (n-1)/2. Must coincide with "num_frames" in the dataset.
    # deformable_groups: 8 # Number of deformable offset groups in the deformable layers
    # n_extract_block: 5 # Number of extract blocks
    # n_reconstruct_block: 10 # Number of reconstruction blocks (M=10, L=40)
    # predeblur: false # Use pre-deblur 
    # tsa: true # Use Temporal Spatial Attention
    
# Discriminator options:
network_D:
    # ESRGAN (default)| PPON:
    which_model_D: patchgan # discriminator_vgg_128 | discriminator_vgg | discriminator_vgg_128_fea (feature extraction) | patchgan | multiscale
    norm_type: batch
    act_type: leakyrelu
    mode: CNA # CNA | NAC
    nf: 64
    in_nc: 3
    nlayer: 3 # only for patchgan and multiscale
    num_D: 3 # only for multiscale

# Schedulers options:
train:
    lr_G: 0.001 # 2e-4 # starting lr_g #Test, default: 1e-4
    weight_decay_G: 0
    beta1_G: 0.9
    lr_D: 0.001 # 2e-4 # starting lr_d #Test, default: 1e-4
    weight_decay_D: 0
    beta1_D: 0.9

    # For MultiStepLR (ESRGAN, default):
    lr_scheme: MultiStepLR
    lr_steps: [80000, 160000] 
    lr_gamma: 0.5 # lr change at every step (multiplied by)

    # For StepLR_Restart (PPON):
    # lr_gamma: 0.9 #lr change at every step (multiplied by)
    # lr_scheme: StepLR_Restart # MultiStepLR | MultiStepLR_Restart | StepLR | StepLR_Restart | CosineAnnealingLR_Restart
    # lr_step_sizes: [200, 100, 250] # Steps for each restart for "StepLR_Restart"
    # restarts: [138000, 172500] # Restart iterations for "MultiStepLR_Restart", "StepLR_Restart" and "CosineAnnealingLR_Restart"
    # restart_weights: [1, 0.5, 0.5] #lr_() * each weight in "restart_weights" for each restart in "restarts"    
    ##clear_state: true
    
    # For MultiStepLR_Restart:
    # lr_gamma: 0.9
    # lr_scheme: MultiStepLR_Restart # MultiStepLR | MultiStepLR_Restart | StepLR | StepLR_Restart | CosineAnnealingLR_Restart
    # lr_steps: [34500, 69000, 103500, 155250, 189750, 241500] #For "MultiStepLR" and "MultiStepLR_Restart"
    # restarts: [138000, 172500] # Restart iterations for "MultiStepLR_Restart", "StepLR_Restart" and "CosineAnnealingLR_Restart"
    # restart_weights: [0.5, 0.5] # lr_() * each weight in "restart_weights" for each restart in "restarts"
    ##clear_state: true

    # For CosineAnnealingLR_Restart (PAN)
    # lr_G: !!float 7e-4
    # lr_scheme: "CosineAnnealingLR_Restart"
    # beta1_G: 0.9
    # beta2_G: 0.99
    # lr_D: 7e-4
    # beta1_D: 0.9
    # beta2_D: 0.99
    # # beta1: 0.9
    # # beta2: 0.99
    # niter: 1000000
    # warmup_iter: -1  # no warm up
    # T_period: [250000, 250000, 250000, 250000]
    # restarts: [250000, 500000, 750000]
    # restart_weights: [1, 1, 1]
    # eta_min: !!float 1e-7
    
    # Losses:
    pixel_criterion: l1 # "l1" | "l2" | "cb" | "elastic" | "relativel1" | "l1cosinesim" | "clipl1" #pixel loss
    pixel_weight: 1 # 1
    feature_criterion: l1 # "l1" | "l2" | "cb" | "elastic" #feature loss (VGG feature network)
    feature_weight: 5e-4
    # cx_weight: 2e-5
    # cx_type: contextual
    # cx_vgg_layers: {conv_3_2: 1, conv_4_2: 1}
    # hfen_criterion: l1 # hfen: "l1" | "l2" | "rel_l1" | "rel_l2" #helps in deblurring and finding edges, lines
    # hfen_weight: 1e-6 
    # grad_type: grad-4d-l1 # 2d | 4d / - any of the pixel crit, ie "grad-2d-l1"
    # grad_weight: 4e-1 # 4e-1
    tv_type: 4D # "normal" | "4D" #helps in denoising, reducing upscale artefacts
    tv_weight: 1e-5 # Change "tv_weight" so the l_g_tv is around 1e-02 - 1e-01
    tv_norm: 1 # 1 for l1 (default) or 2 for l2.
    # ssim_type: ssim # "ssim" | "ms-ssim" #helps to maintain luminance, contrast and covariance between SR and HR
    # ssim_weight: 1
    lpips_weight: 1e-2 # perceptual loss
    lpips_type: net-lin # net-lin | net *
    lpips_net: squeeze # "vgg" | "alex" | "squeeze" 
    
    # Experimental losses
    # spl_type: spl # "spl" | "gpl" | "cpl"
    # spl_weight: 0.1 # 1e-2 # SPL loss function. note: needs to add a cap in the generator (finalcap; For [0,1] range -> "finalcap": "clamp") or the overflow loss or it can become unstable.
    # of_type: overflow # overflow loss function to force the images back into the [0, 1] range
    # of_weight: 0.2
    # fft_type: fft
    # fft_weight: 0.1
    # color_criterion: color-l1cosinesim # l1cosinesim naturally helps color consistency, so it is the best to use here, but others can be used as well
    # color_weight: 1 # Loss based on the UV channels of YUV color space, helps preserve color consistency
    # avg_criterion: avg-l1
    # avg_weight: 5 # Averaging downscale loss
    # ms_criterion: multiscale-l1
    # ms_weight: 1e-2

    # video losses
    ofr_type: ofr #optical flow reconstruction
    ofr_weight: [0.1, 0.2, 0.1, 0.01] #SOFVSR defaults: lambda1 = 0.1 (loss_L1), lambda2 = 0.2 (loss_L2), lambda3 = 0.1 (regularization), lambda4 = 0.01 (total)
    
    # Adversarial loss:
    gan_type: vanilla # "vanilla" | "wgan-gp" | "lsgan" 
    gan_weight: 5e-5 # 
    # for wgan-gp
    # D_update_ratio: 1
    # D_init_iters: 0
    # gp_weigth: 10
    # if using the discriminator_vgg_128_fea feature maps to calculate feature loss
    # gan_featmaps: true # true | false
    # dis_feature_criterion: cb # "l1" | "l2" | "cb" | "elastic" #discriminator feature loss
    # dis_feature_weight: 0.01 # 1
        
    # Differentiable Augmentation for Data-Efficient GAN Training
    # diffaug: true
    # dapolicy: 'color,transl_zoom,flip,rotate,cutout' # smart "all" (translation, zoom_in and zoom_out are exclusive)
    
    # Frequency Separator
    # fs: true
    # lpf_type: average # "average" | "gaussian"
    # hpf_type: average # "average" | "gaussian"
    
    # Other training options:
    # finalcap: clamp # Test. Cap Generator outputs to fit in: [-1, 1] range ("tanh"), rescale tanh to [0,1] range ("scaltanh"), cap ("sigmoid") or clamp ("clamp") to [0,1] range. Default = None. Coordinate with znorm. Required for SPL if using image range [0,1]
    manual_seed: 0
    niter: 2e5
    val_freq: 100000000 # 5e3
    # overwrite_val_imgs: true
    # val_comparison: true
    metrics: 'psnr,ssim,lpips' # select from: "psnr,ssim,lpips" or a combination separated by comma ","

logger:
    print_freq: 200
    save_checkpoint_freq: 5e3
    overwrite_chkp: false
